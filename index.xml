<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>伟锅的博客</title>
    <link>https://qingsimon.github.io/</link>
    <description>Recent content on 伟锅的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 26 Sep 2018 13:44:52 +0800</lastBuildDate>
    
        <atom:link href="https://qingsimon.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>https://qingsimon.github.io/about/</link>
      <pubDate>Wed, 26 Sep 2018 13:44:52 +0800</pubDate>
      
      <guid>https://qingsimon.github.io/about/</guid>
      
        <description>&lt;p&gt;怕什么真理无穷，进一寸有进一寸的欢喜！&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>SLAM：现在，未来和鲁棒感知时代</title>
      <link>https://qingsimon.github.io/post/2018-10-12-slam-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5-%E5%92%8C-%E9%B2%81%E6%A3%92%E6%84%9F%E7%9F%A5%E6%97%B6%E4%BB%A3/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qingsimon.github.io/post/2018-10-12-slam-%E7%8E%B0%E5%9C%A8-%E6%9C%AA%E6%9D%A5-%E5%92%8C-%E9%B2%81%E6%A3%92%E6%84%9F%E7%9F%A5%E6%97%B6%E4%BB%A3/</guid>
      
        <description>

&lt;p&gt;本文为对综述论文&lt;a href=&#34;http://xueshu.baidu.com/s?wd=paperuri:%28037161a22e6c2c4511d22876ffe493cd%29&amp;amp;filter=sc_long_sign&amp;amp;sc_ks_para=q=Simultaneous%20Localization%20And%20Mapping:%20Present,%20Future,%20and%20the%20Robust-Perception%20Age&amp;amp;sc_us=345543128033019768&amp;amp;tn=SE_baiduxueshu_c1gjeupa&amp;amp;ie=utf-8&#34;&gt;Simultaneous Localization And Mapping: Present, Future, and the Robust-Perception Age&lt;/a&gt;的理解和心得，本文受众为，对SLAM问题有一个全面的基础认知，想进一步寻找SLAM研究方向的童鞋。
对该综述论文的学习还可以参考：&lt;br /&gt;
&lt;a href=&#34;https://mp.weixin.qq.com/s/mIdRE5r7matHfokOrinXzw&#34;&gt;【泡泡机器人翻译专栏】SLAM: 现在，未来和鲁棒年代（一）&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://mp.weixin.qq.com/s/P6reSIbdV0--WMkq3qSx1A&#34;&gt;【泡泡机器人翻译专栏】SLAM: 现在，未来和鲁棒年代（二）&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://mp.weixin.qq.com/s/P3t38nX5ZNW4qEuUvHRmkQ&#34;&gt;【泡泡机器人翻译专栏】SLAM: 现在，未来和鲁棒年代（三）&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://mp.weixin.qq.com/s/7C_2q-TLuXlh8NLTZFwVFQ&#34;&gt;【泡泡机器人翻译专栏】SLAM: 现在，未来和鲁棒年代（四）&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://mp.weixin.qq.com/s/AoOpgw6LqRondo6JdizRSw&#34;&gt;【泡泡机器人翻译专栏】SLAM: 现在，未来和鲁棒年代（五）&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;slam-研究的时代划分&#34;&gt;SLAM 研究的时代划分&lt;/h1&gt;

&lt;p&gt;Cadena等人把SLAM问题的研究划分为3个时代：
1.  the classical age (1986 - 2004)
2.  the algorithmic-analysis age (2004 - 2015)
3.  the robust-perception age (now)&lt;/p&gt;

&lt;h2 id=&#34;the-classical-age-1986-2004&#34;&gt;the classical age (1986 - 2004)&lt;/h2&gt;

&lt;p&gt;在SLAM问题研究的前20年，主要是提出问题和理清框架，这一阶段的主要有两个成果：
1. 对SLAM问题，引入了一些主要的概率形式表述（probabilistic formulation）和框架，包括基于扩展卡尔曼滤波器（Extended Kalman Filters），Rao-Blackwellised 粒子滤波器（Rao-Blackwellised Partical Filter）和极大似然估计的方法（maximum likelihood estimation）；
2.  指出了SLAM问题在效率（efficiency）和鲁棒的数据关联（robust data association）方面面对的挑战。&lt;/p&gt;

&lt;h2 id=&#34;the-algorithmic-analysis-age-2004-2015&#34;&gt;the algorithmic-analysis age (2004 - 2015)&lt;/h2&gt;

&lt;p&gt;这一阶段对SLAM问题进行算法上的分析，主要成果为：
1.  研究了SLAM问题的基本性质，包括可观测性（observability），收敛性（convergence）和一致性（consistency）；
2. 理解了SLAM问题的稀疏性（sparsity），更具体一点，是理解了增量方程中H矩阵结构的稀疏性，这对于SLAM的效率起着至关重要的作用。
3.  一些主要的开源SLAM框架也在这一阶段被开发了出来。&lt;/p&gt;

&lt;h2 id=&#34;the-robust-perception-age-now&#34;&gt;the robust-perception age (now)&lt;/h2&gt;

&lt;p&gt;Cadena等人认为，在我们正处于的这个时代，SLAM领域的研究有以下特点：&lt;/p&gt;

&lt;h1 id=&#34;自主机器人真的需要slam吗&#34;&gt;自主机器人真的需要SLAM吗？&lt;/h1&gt;

&lt;h1 id=&#34;slam问题被解决了吗&#34;&gt;SLAM问题被解决了吗？&lt;/h1&gt;

&lt;p&gt;在我初学SLAM的时候，这是一个很困扰的问题，因为一些现有的开源框架如ORB-SLAM2、SVO、DSO和、OKVIS、VINS-Mono等，在算法框架上已经很成熟了，而且在数据集上也有着很好的表现，似乎剩下的工作就只是将它们工程化、实用化，根据实际应用需求进行微小改动而已。
Cadena等人认为。&lt;/p&gt;

&lt;h1 id=&#34;slam算法鲁棒性&#34;&gt;SLAM算法鲁棒性&lt;/h1&gt;

&lt;h2 id=&#34;slam算法鲁棒性面临的主要挑战&#34;&gt;SLAM算法鲁棒性面临的主要挑战&lt;/h2&gt;

&lt;h3 id=&#34;数据关联-data-association&#34;&gt;数据关联（data association）&lt;/h3&gt;

&lt;p&gt;数据关联错误，是导致SLAM算法失效的一个主要原因。数据关联把观测值和相应的状态进行关联。以特征点法视觉SLAM为例，它把每一个特征点与相应的路标点进行关联。感知混淆（perceptual aliasing，感知混淆是指，对于不同的输入，传感器感知到了相同信号的现象）使得正确的数据关联变得特别困难。感知混淆会导致数据关联建立错误的观测-状态（measurement-state）匹配关系（false positive，假阳性），这会导致后端优化作出错误的状态估计。&lt;/p&gt;

&lt;p&gt;如果没有对环境的动态性（包括短期变化和长期的季节性变化）进行建模，数据关联错误的情况会很严重。当前的SLAM系统，通常都会假设机器人所处的环境是静态的。对于小尺度场景下的单次建图，如果环境在短期内没有动态性（比如没有人或者其它物体在周围移动），那么静态环境假设（static world assumption）是可取的。但是如果在更长的时间尺度和更大的空间尺度上进行建图，环境的变化是不可避免的，这种情况下静态环境假设就不合理了。环境的变化来自很多方面：昼夜变化，季节变化（比如树叶的变化），环境结构的变化（比如旧建筑物翻新）。这些环境变化都会影响SLAM系统的性能。比如：昼夜极端的光照变化，会使得依赖视觉特征复现性的SLAM系统失效；旧的环境结构消失，会导致基于环境几何结构的SLAM系统失效。&lt;/p&gt;

&lt;h3 id=&#34;恶劣环境-harsh-environment&#34;&gt;恶劣环境（harsh environment）&lt;/h3&gt;

&lt;p&gt;SLAM算法鲁棒性面临的另一个挑战，是其在恶劣环境下的运行，比如水下环境。这些恶劣环境下面临的挑战是：低能见度（low visibility），持续变化的环境，以及无法使用一些常用传感器（如激光雷达等）。&lt;/p&gt;

&lt;h2 id=&#34;研究现状&#34;&gt;研究现状&lt;/h2&gt;

&lt;h3 id=&#34;前端短期数据关联-short-term-data-association&#34;&gt;前端短期数据关联（short-term data association）&lt;/h3&gt;

&lt;p&gt;由数据关联错误造成的鲁棒性问题，既可以在SLAM系统的前端进行处理，也可以在SLAM系统的后端处理。一般来说，SLAM前端建立的数据关联是可信的。短期的数据关联是最容易处理的：如果传感器的采样频率，相比于机器人自身的运动足够快的话，那么对于同一3D路标的跟踪就比较容易。比如，我们想从连续图像中跟踪一个3D点，且假设图像的帧率足够高，那么基于描述子的普通匹配方法，或者光流法就可以实现可靠的跟踪。直观上讲，在高帧率情况下，传感器（摄像头、雷达）的视角变化不大，因此t+1时刻的特征与t时刻的特征非常接近（相比于长期数据关联，短期的数据关联问题更加简单，关联结果更加可靠，这也是视觉/惯性里程计比SLAM简单的原因）。&lt;/p&gt;

&lt;h3 id=&#34;前端长期数据关联-long-term-data-association&#34;&gt;前端长期数据关联（long-term data association）&lt;/h3&gt;

&lt;p&gt;在SLAM前端进行长期数据关联更具有挑战性，它包括回环闭合的检测和验证（loop closure detection and validation）。&lt;/p&gt;

&lt;h4 id=&#34;前端回环闭合检测-loop-closure-detection&#34;&gt;前端回环闭合检测（loop closure detection ）&lt;/h4&gt;

&lt;p&gt;对于SLAM前端的回环闭合检测，一个比较直观的方法就是，在当前观测（比如图像）中检测特征，将其之前所有检测到的特征进行对比，但考虑到计算量，这种暴力求解方法是不切实际的。词袋模型（bag-of-words）通过对特征空间进行量化，从而实现了更加有效的搜索，避免了暴力搜索。词袋模型可以设计为层级式字典树（hierarchical vocabulary tree），这种结构可以实现大规模数据集中的高效查找。&lt;/p&gt;

&lt;p&gt;基于词袋模型的方法，在单次任务回环闭合检测中表现出了非常可靠的性能。但是这些方法无法处理剧烈的光照变化，因为光照剧烈变化时，就无法与字典树中的视觉单词（visual word）匹配上了。为了处理这个问题，有的方法匹配图像序列，有的方法把不同的视觉特征整合为同一的表示，还有的方法同时使用空间信息和视觉特征，视觉场景识别的详细内容可参考综述论文&lt;a href=&#34;http://xueshu.baidu.com/s?wd=paperuri:%283576dfee40a2d971c2e6d2042979f502%29&amp;amp;filter=sc_long_sign&amp;amp;sc_ks_para=q=Visual%20Place%20Recognition:%20A%20Survey&amp;amp;sc_us=9574256859953260926&amp;amp;tn=SE_baiduxueshu_c1gjeupa&amp;amp;ie=utf-8&#34;&gt;Visual Place Recognition: A Survey&lt;/a&gt;。在基于激光雷达的SLAM前端，基于特征的方法也可以用来进行回环闭合检测。&lt;/p&gt;

&lt;h4 id=&#34;前端回环闭合验证-loop-closure-validation&#34;&gt;前端回环闭合验证（loop closure validation）&lt;/h4&gt;

&lt;p&gt;回环闭合验证，指的是通过额外的几何验证步骤来确保回环闭合的质量。在基于视觉的方法中，通常使用RANSAC进行几何校验，剔除离群点（outlier）。在基于激光雷达的方法中，激光雷达通过之前的扫描已经建出了地图，可以通过计算当前激光雷达的扫描值与地图的匹配度，来完成对回环闭合的验证。&lt;/p&gt;

&lt;h3 id=&#34;后端回环闭合验证&#34;&gt;后端回环闭合验证&lt;/h3&gt;

&lt;p&gt;尽管可以通过各种方法去增强SLAM前端在回环闭合检测中的鲁棒性，错误的回环闭合检测还是无法被完全避免，比如出现感知混淆（perceptual aliasing）的时候。SLAM后端获取错误的回环闭合检测信息，会严重影响后端极大后验估计（MAP）的质量。为了处理这个问题，有一些研究提出了一些方法，可以使SLAM后端对于前端传递过来的错误回环闭合信息更加鲁棒。有些方法在后端优化中添加与回环闭合相关的残差项，通过残差值推断回环闭合检测的正确性。在有一些方法中，与里程计数据相悖的回环闭合检测结果被认为是错误的回环检测，进而在后端优化之前，获取离群点（outlier）的先验信息。&lt;/p&gt;

&lt;h3 id=&#34;动态环境的处理&#34;&gt;动态环境的处理&lt;/h3&gt;

&lt;p&gt;动态环境对SLAM的挑战主要有两方面：
1. SLAM系统需要检测、丢弃和跟踪环境中的变化。目前的主流方法选择剔除环境中动态变化的部分，但也有一些研究把动态变化的元素作为环境的一部分进行建模。
2.  SLAM系统需要对环境中的永久变化（permanent change）或半永久变化（permanent change）进行建模，系统需要理解什么时候应该更新地图，以及该如何更新地图。现有的SLAM系统在处理动态场景时主要有两种方式：一是维护同一个位置的多个地图；二是只维护一个地图，但是构建地图的参数是时变的。&lt;/p&gt;

&lt;h2 id=&#34;待解决的问题-open-problem&#34;&gt;待解决的问题（open problem）&lt;/h2&gt;

&lt;h3 id=&#34;slam失效保护和恢复-failsafe-slam-and-recovery&#34;&gt;SLAM失效保护和恢复（failsafe SLAM and recovery）&lt;/h3&gt;

&lt;p&gt;尽管SLAM后端已经取得了很大进步，现在的SLAM系统求解器在出现离群点（outlier）时依然很脆弱。这主要是因为，几乎所有的鲁棒SLAM方案都是基于非凸目标函数的迭代优化，这会导致两个结果：第一，离群点的去除效果取决于优化过程中初始估计的质量；第二，系统具有内生的脆弱性：后端优化即使只使用了一个离群点，都会降低状态估计的质量，糟糕的状态估计又会反过来降低系统剔除后续离群点的能力。&lt;/p&gt;

&lt;p&gt;一个理想的SLAM系统应该具有失效保护和失效侦测的能力，系统必须能检测到突发的失效情况（比如由离群点或者硬件老化造成的失效），要有恢复机制进行适当操作，使得系统能够重新正常工作。现有的SLAM系统没有这些功能。&lt;/p&gt;

&lt;h3 id=&#34;对硬件失效的鲁棒性-robustness-to-hw-failure&#34;&gt;对硬件失效的鲁棒性（robustness to HW failure）&lt;/h3&gt;

&lt;p&gt;解决硬件失效问题并不属于SLAM的研究范畴，但硬件失效会影响SLAM系统，SLAM系统反过来会对，检测、减少传感器失效和运动失效，发挥重要作用。如果传感器精度由于硬件故障或老化而降低，那么传感器的测量（包含噪声和偏置），就不再与后端优化中所使用的传感器噪声模型相匹配，这会导致较差的估计结果。这就引出了各种需要研究的问题：如何检测传感器的不良运行状态？如何相应地调整传感器噪声模型中的协方差和偏置？&lt;/p&gt;

&lt;h3 id=&#34;几何度量重定位-metric-relocalization&#34;&gt;几何度量重定位（metric relocalization）&lt;/h3&gt;

&lt;p&gt;几何度量重定位，是指估计机器人当前时刻，在已建地图中的位姿。相对于基于特征（feature-based）的重定位，基于外观（appearance-based）的重定位能在昼夜、不同的季节之间实现回环闭合检测，但得到的回环闭合结果，本质上是拓扑的，是没有几何度量信息的。对于几何度量重定位来说，基于特征的方法仍然是最常用的，而这种方法不能被扩展到昼夜、不同季节场景下的重定位。&lt;/p&gt;

&lt;p&gt;目前很多SLAM系统都选择视觉传感器（摄像头），魂环闭合问题变成了一个传感器信号匹配的问题。尽管摄像头是主要的传感器，其它的传感器数据和SLAM系统中的一些其它信息也可以用于帮助重定位。Brubaker等人使用轨迹（trajectory）匹配来克服相机的缺陷；先使用一种传感器模式进行建图，然后再建好的图中使用另一种传感器模式进行重定位，这也是一个研究方向。&lt;/p&gt;

&lt;p&gt;另一个问题是，如何根据不同来源、不同视角的传感器数据进行定位。Forster等人研究了在激光雷达地图中的视觉定位问题。Majdik等人研究了如何在Google街景图片的3D纹理图上定位无人机。Behzadin等人展示了如何使用激光扫描仪在手绘地图中定位。Winterhalter等人演示了如何在给定的2D户型图上，使用RGB-D传感器进行定位。&lt;/p&gt;

&lt;h3 id=&#34;随时间变化的-可变形的地图-time-varing-and-deformable-map&#34;&gt;随时间变化的、可变形的地图（time varing and deformable map）&lt;/h3&gt;

&lt;p&gt;主流SLAM系统都假设世界是静止的，然而由于物体的动态变化，以及物体天然的变形特性，真实世界不是一成不变的。一个理想的SLAM方案应该能够处理环境中的非刚体性（non-rigidity）等动态因素，能够长时间工作生成全地形图（&amp;rdquo;all terrain map&amp;rdquo;），而且能够实时完成这些工作。&lt;/p&gt;

&lt;p&gt;在计算机视觉领域，从80年代起就有研究尝试恢复非刚性物体的形状，但它们只能在限定条件下使用。比如Pentland等人的论文需要知道关于目标力学性质的先验性息。Bregler等人的论文需要对目标的变形进行限定，他们展示了人脸形状恢复的例子。近期的非刚体SFM研究成果，限定条件减少了，但仍然只能应用于小场景。在SLAM领域，Newcombe等人解决了非刚体场景下的小规模重建问题。然而，非刚体场景的大规模重建问题仍有待研究。&lt;/p&gt;

&lt;h3 id=&#34;自动化调参-automatic-parameter-tuning&#34;&gt;自动化调参（Automatic parameter tuning）&lt;/h3&gt;

&lt;p&gt;SLAM系统（尤其是数据关联模块）需要大量的参数调整，以保证其在给定的场景中能够正常工作。这些参数包括控制特征匹配的阈值，RANSAC参数，向因子图中添加新因子的标准，判断回环闭合的标准。如果想要SLAM系统不经手动调整，就能在各种场景下使用，那就需要考虑参数自动调整。&lt;/p&gt;

&lt;h1 id=&#34;slam的场景尺度-scalability&#34;&gt;SLAM的场景尺度（scalability）&lt;/h1&gt;

&lt;p&gt;现代SLAM系统已经成功地应用于室内场景。&lt;/p&gt;

&lt;h2 id=&#34;研究现状-1&#34;&gt;研究现状&lt;/h2&gt;

&lt;h3 id=&#34;节点-边缘稀疏化-node-and-edge-sparsification&#34;&gt;节点、边缘稀疏化（node and edge sparsification）&lt;/h3&gt;

&lt;h3 id=&#34;去中心化-并行-slam-out-of-core-parallel-slam&#34;&gt;去中心化（并行）SLAM（out-of-core (parallel) SLAM）&lt;/h3&gt;

&lt;h3 id=&#34;分布式多机器人slam-distributed-multi-robot-slam&#34;&gt;分布式多机器人SLAM（distributed multi robot SLAM）&lt;/h3&gt;

&lt;h2 id=&#34;待解决的问题&#34;&gt;待解决的问题&lt;/h2&gt;

&lt;h3 id=&#34;地图维护-map-maintenance&#34;&gt;地图维护（map maintenance）&lt;/h3&gt;

&lt;h3 id=&#34;鲁棒分布式建图-robust-distributed-mapping&#34;&gt;鲁棒分布式建图（robust distributed mapping）&lt;/h3&gt;

&lt;h3 id=&#34;学习-遗忘-记忆-learning-forgetting-remembering&#34;&gt;学习，遗忘，记忆（learning, forgetting, remembering）&lt;/h3&gt;

&lt;h3 id=&#34;资源有限的平台-resource-constrained-platform&#34;&gt;资源有限的平台（resource-constrained platform）&lt;/h3&gt;

&lt;h1 id=&#34;地图表示-几何度量推理-metric-reasoning&#34;&gt;地图表示：几何度量推理（metric reasoning）&lt;/h1&gt;

&lt;p&gt;几何度量地图，是编码环境几何信息的符号结构（symbolic structure）。在SLAM中，如何为SLAM选择合适的几何度量地图（或者扩展机器人当前所使用的几何度量地图），会影响许多研究领域，包括长时间导航、与环境的物理交互、人机交互。&lt;/p&gt;

&lt;p&gt;在2D情况下，构建环境的几何模型相对简单，只有两种主要范式：
1. 路标地图（landmark-based maps）。在路标地图中，环境被建模为路标的稀疏集合
2. 栅格地图（occupancy grid maps）。在栅格地图中，环境被离散化为许多栅格，每一个栅格会被赋予一个概率值，该值表征了这个栅格被占据的概率。&lt;/p&gt;

&lt;p&gt;2D环境中地图的标准化问题，已经被IEEE RAS Map Data Representation Working Group解决了，可参见他们的论文&lt;a href=&#34;http://xueshu.baidu.com/s?wd=paperuri:%28ca0ffad637b4d46a0aab1af4ad348850%29&amp;amp;filter=sc_long_sign&amp;amp;sc_ks_para=q=IEEE%20Standard%20for%20Robot%20Map%20Data%20Representation%20for%20Navigation&amp;amp;sc_us=9968457593541213647&amp;amp;tn=SE_baiduxueshu_c1gjeupa&amp;amp;ie=utf-8&#34;&gt;IEEE Standard for Robot Map Data Representation for Navigation&lt;/a&gt;。这个标准定义了平面环境的两种几何度量地图表示（还有拓扑地图），用于数据交换（data exchange），参照基准（benchmarking）和技术转移（techonology transfer）。&lt;/p&gt;

&lt;p&gt;3D环境的集合建模相对复杂（delicate），如何有效地对3D环境进行几何建模，仍处于研究的早期阶段。下面会以一个跨越机器人学（robotics）、计算机视觉（computer vision）、计算机辅助设计（CAD, computer aided design）和计算机图形学（computer graphics）的广阔视角，回顾3D环境的几何建模方法。&lt;/p&gt;

&lt;h2 id=&#34;稀疏路标表示-landmark-based-sparse-representation&#34;&gt;稀疏路标表示（landmark-based sparse representation）&lt;/h2&gt;

&lt;h2 id=&#34;低层原始数据稠密表示-low-level-raw-dense-representation&#34;&gt;低层原始数据稠密表示（low-level raw dense representation）&lt;/h2&gt;

&lt;h2 id=&#34;边界和空间分割稠密表示-boundary-and-spatial-partitioning-dense-representation&#34;&gt;边界和空间分割稠密表示（boundary and spatial-partitioning dense representation）&lt;/h2&gt;

&lt;h2 id=&#34;基于物体的高阶表示-high-level-object-based-representation&#34;&gt;基于物体的高阶表示（high-level object-based representation）&lt;/h2&gt;

&lt;h2 id=&#34;待解决的问题-open-problem-1&#34;&gt;待解决的问题（open problem）&lt;/h2&gt;

&lt;p&gt;以下这些关于SLAM中几何度量表示的问题，需要大量的基础研究，仍然有很多未知领域没有开发。&lt;/p&gt;

&lt;h3 id=&#34;slam中的高层表示-high-level-expressive-representations-in-slam&#34;&gt;SLAM中的高层表示（high-level, expressive representations in SLAM）&lt;/h3&gt;

&lt;h3 id=&#34;最优表示-optimal-representations&#34;&gt;最优表示(optimal representations)&lt;/h3&gt;

&lt;h3 id=&#34;自动-自适应表示-automatic-adaptive-represetation&#34;&gt;自动、自适应表示（automatic,  adaptive represetation）&lt;/h3&gt;

&lt;h1 id=&#34;地图表示-语义推理-semantic-reasoning&#34;&gt;地图表示：语义推理（semantic reasoning）&lt;/h1&gt;

&lt;p&gt;语义建图（semantic mapping）需要将语义信息与机器人所处环境中的几何实体进行关联。最近，人们已经意识到了纯粹几何地图的局限性，在构建环境语义地图方面产生了很多的重要工作。语义地图可以提升机器人的自主性和鲁棒性、使机器人能够处理更加复杂的任务（比如在行驶过程中避免泥泞道路）、从路径规划（path-planning）变成任务规划（task-planning），以及实现高级的人机交互。在语义地图方面，有着大量不同的方法，它们所使用语义信息的数量和类别不尽相同，也使用不同的方法将语义信息与环境中的不同部分进行关联。此外，也有一些方法将低层的语义分析表述为一个分类问题，它们考虑的是传感器数据与语义信息之间的简单映射。&lt;/p&gt;

&lt;h2 id=&#34;语义slam-vs-拓扑slam-semantic-slma-vs-topological-slam&#34;&gt;语义SLAM vs 拓扑SLAM（semantic SLMA vs topological SLAM）&lt;/h2&gt;

&lt;p&gt;拓扑建图舍弃了几何度量信息，仅仅通过场景识别来构建图（graph，图论中的图），图中的节点（node）表示不同的位置，图中的边（edge）表示不同位置之间的连通情况。语义建图与拓扑建图完全不同，拓扑建图需要识别之前看到过的地方（不管它到底是厨房、走廊、或者别的什么），而语义建图则需要根据语义标签对这些位置进行分类。&lt;/p&gt;

&lt;h2 id=&#34;语义的结构和具体内容-semantic-slam-structure-and-detail-of-concepts&#34;&gt;语义的结构和具体内容（semantic SLAM: structure and detail of concepts）&lt;/h2&gt;

&lt;p&gt;对于人类来说，可能有无数的概念，这些概念之间也有着无数的相互关系，但是人类是任务驱动的（task-driven），根据任务需求，人类可以更加睿智的去选择不同的层级和组织结构来使用这些概念。人类的这种特性，对于构建智能机器人是有借鉴意义的。语义的具体内容和组织方式，取决于机器人所处的环境，以及机器人所需要执行的任务。而语义的具体内容和组织方式，会在不同的阶段影响问题的复杂性。因此，构建语义地图需要确定以下两方面的内容：
1. 语义的层级和具体内容（level/detail of semantic concepts）。对于一个给定的机器人任务，比如“从房间A到房间B”，初步的分类（比如房间、走廊、门）就可以很好的完成这个任务。但是对于其它的一些任务，比如“拿起一个茶杯”，就需要更加精细的分类（桌子、茶杯、玻璃杯）。
2. 语义的组织形式（organization of semantic concepts）。语义信息并不是唯一的（exclusive），多个语义信息可以表达同一个实体（比如凳子、板凳），一个语义信息也可以表达多个实体（比如桌子一个词，既可以指课桌，也可以指饭桌）。一个实体可能有无穷多的属性。一个椅子的属性可能是“可移动的”和“可坐”，一个餐桌的属性可能是“可移动”和“不可坐”。椅子和餐桌都是家具，它们都具有“可移动”的共同属性，但是具有不同的用途。由于语义存在的这些特点，我们在组织语义信息时，不管使用扁平结构还是层级结构（flat or hierarchical organization），无论有没有共同属性，都必须要能够处理语义信息的多样性（multiplicity）。&lt;/p&gt;

&lt;h2 id=&#34;语义建图的研究现状&#34;&gt;语义建图的研究现状&lt;/h2&gt;

&lt;p&gt;在语义建图或赋予数据语义信息方面，目前有3中主要方法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;SLAM帮助语义（SLAM helps semantics）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;语义帮助SLAM（semantics help SLAM）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;SLAM与语义联合推理（joint SLAM and semantic inference）&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;待解决的问题-open-problem-2&#34;&gt;待解决的问题（open problem）&lt;/h2&gt;

&lt;p&gt;语义SLAM不像几何度量SLAM那样有一套成熟的框架，而是尚处于研究的早期阶段。&lt;br /&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
    &lt;img src=&#34;https://qingsimon.github.io/img/ConstructionSite.png&#34; alt=&#34;&#34; width=&#34;800&#34;/&gt;
&lt;/div&gt;
人类通过语义理解，可以预测环境在不同时间尺度下的变化情况。以上图中中的一个工地为例，我们人类可以理解图片底部的吊车的运动，并且可以预测它在短期内不会移动，同时我们还可以预测工地在施工完成后的外形，这样在施工结束后，我们依然可以在完成自己在工地中的定位。这得益于人类可以对环境中目标的功能以及它们之间的相互关系进行推理，使机器人能够具备相似的能力是语义SLAM中有待解决的问题。从工地的例子中，可以发现语义SLAM面临的以下挑战。&lt;/p&gt;

&lt;h3 id=&#34;语义建图不只是一个分类问题&#34;&gt;语义建图不只是一个分类问题&lt;/h3&gt;

&lt;p&gt;语义概念要被处理为更加特定的信息，比如地图中实体的affordance和actionability（affordance指的是，对于给定的环境或者物体，给定的agent可能采取的动作的集合，actionability指的是这些动作的预期效用），以及环境中不同agent之间可能产生的交互。如何表示这些属性和相互关系，是高层的人机交互需要解决的问题。&lt;/p&gt;

&lt;h3 id=&#34;忽略-察觉和适应-ignorance-awareness-and-adaptation&#34;&gt;忽略、察觉和适应（ignorance, awareness, and adaptation）&lt;/h3&gt;

&lt;p&gt;给定一些先验知识，机器人要能够推理出新的概念和它们的语义表示。也就是说，机器人要能够发现环境中新的目标和类别，在与其它机器人或人类的交互中学习新的属性，对环境中缓慢的或者突然的变化，自适应地采取相应的表示方法。&lt;/p&gt;

&lt;p&gt;举个例子，一个轮式机器人需要分辨地形是否可行驶，然后通知导航系统。如果机器人发现道路上有泥巴，根据之前的分类结果，道路是可行驶的，机器人需要根据穿越这个泥泞地形的难度学习出一个新的类别，如果机器人察觉到有其它车辆陷在了泥泞中，那么它也要相应地调整它的分类器。&lt;/p&gt;

&lt;h3 id=&#34;基于语义的推理&#34;&gt;基于语义的推理&lt;/h3&gt;

&lt;p&gt;对于人类而言，我们可以利用语义信息，对环境进行压缩，以及加速对环境的推理，而确定精确的集合度量对人类来说是很困难的事情。然而对于目前的机器人来说，却不是这样，机器人可以处理（带有颜色信息）的集合度量表示，但它们不能真正地利用语义信息。目前我们的机器人还不能够通过环境中的语义信息（类别、相互关系和属性），进行有效的、高效的定位和持续建图。&lt;/p&gt;

&lt;p&gt;举个例子：当机器人探测到一辆汽车时，它应该能够推理出汽车下的 地面（即使有遮挡），而且当这辆汽车移动时，机器人的传感器会获取新的数据，地图更新应该可以优化之前猜测的地面。甚至，在优化地面的同时，机器人应该能够通过单一有效的操作，将汽车作为一个整体更新其全局坐标，而不是更新每一个单一的体素。&lt;/p&gt;

&lt;h1 id=&#34;slam中的新理论工具&#34;&gt;SLAM中的新理论工具&lt;/h1&gt;

&lt;h1 id=&#34;主动slam-active-slam&#34;&gt;主动SLAM（active SLAM）&lt;/h1&gt;

&lt;h1 id=&#34;slam中的新传感器&#34;&gt;SLAM中的新传感器&lt;/h1&gt;
</description>
      
    </item>
    
    <item>
      <title>自动驾驶中的传感器融合</title>
      <link>https://qingsimon.github.io/post/2018-10-12-%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qingsimon.github.io/post/2018-10-12-%E5%A4%9A%E4%BC%A0%E6%84%9F%E5%99%A8%E8%9E%8D%E5%90%88/</guid>
      
        <description>

&lt;h2 id=&#34;多传感器时间同步&#34;&gt;多传感器时间同步&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34; https://zhuanlan.zhihu.com/p/34982463&#34; target=&#34;_blank&#34;&gt;Momenta Paper Reading：传感器网络的时间同步&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;多传感器空间同步&#34;&gt;多传感器空间同步&lt;/h2&gt;

&lt;h2 id=&#34;多传感器后融合&#34;&gt;多传感器后融合&lt;/h2&gt;

&lt;p&gt;后融合是较为常用的融合方式，可以理解为松耦合，相对简单。&lt;/p&gt;

&lt;h2 id=&#34;多传感器前融合&#34;&gt;多传感器前融合&lt;/h2&gt;

&lt;p&gt;前融合可以理解为紧耦合，相对复杂。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/37866364&#34; target=&#34;_blank&#34;&gt;多传感器前融合&lt;/a&gt;对Roadstar的传感器前融合技术DeepFusion有较为详细的介绍。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>semantic SLAM</title>
      <link>https://qingsimon.github.io/post/2018-10-11-semantic-slam/</link>
      <pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qingsimon.github.io/post/2018-10-11-semantic-slam/</guid>
      
        <description>&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/luyb/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;路游侠的博客&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt;
路游侠的博客里有一些关于语义SLAM数据关联的文章。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/slampaper&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;语义SLAM基础与论文解析&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.liuxiao.org/2018/08/semantic-slam-%E6%96%87%E7%AB%A0%E6%94%B6%E9%9B%86/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Semantic SLAM 文章收集&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/43192675&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;语义SLAM最新论文主要创新点整理&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/46191759&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;[论文笔记] VSO: Visual Semantic Odometry&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/46187281&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;语义SLAM论文阅读总结&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>VINS-Mono前端特征点跟踪</title>
      <link>https://qingsimon.github.io/post/2018-09-29-vins-mono%E5%89%8D%E7%AB%AF%E7%89%B9%E5%BE%81%E7%82%B9%E8%B7%9F%E8%B8%AA/</link>
      <pubDate>Sat, 29 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qingsimon.github.io/post/2018-09-29-vins-mono%E5%89%8D%E7%AB%AF%E7%89%B9%E5%BE%81%E7%82%B9%E8%B7%9F%E8%B8%AA/</guid>
      
        <description>

&lt;h1 id=&#34;vins-mono前端概述&#34;&gt;VINS-Mono前端概述&lt;/h1&gt;

&lt;p&gt;VINS-Mono将前端封装为一个ROS节点feature_tracker，该节点订阅相机图像话题数据后，提取特征点（cv::GoodFeatureToTrack()检测的角点），然后用KLT光流进行特征点跟踪。feature_tracker节点将跟踪的特征点作为话题进行发布，供后端ROS节点使用。同时feature_tracker_node还会发布标记了特征点的图片，可供Rviz显示以供调试。&lt;/p&gt;

&lt;p&gt;前端节点的实现在feature_tracker目录下的src中，src里共有3个头文件和3个源文件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;tic_toc.h中是作者自己封装的一个类TIC_TOC，用来计时；&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;parameters.h和parameters.cpp处理前端中需要用到的一些参数；&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;feature_tracker.h和feature_tracker.cpp实现了一个类FeatureTracker，用来完成特征点提取和特征点跟踪等主要功能；&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;feature_tracker_node.cpp构造了一个ROS节点feature_tracker_node，主要调用FeatureTracker类来实现前端功能。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;入口函数main&#34;&gt;入口函数main()&lt;/h1&gt;

&lt;p&gt;前端的入口函数为feature_tracker_node.cpp中的main()函数。在main()函数中，首先创建名为“feature_tracker”的节点，然后调用parameters.cpp中定义的函数readParameters()，读取特征点提取和跟踪需要用到的一些配置参数。&lt;/p&gt;

&lt;p&gt;在feature_tracker_node.cpp的开头，main()函数之外，会创建由FeatureTracker类的实例组成的数组trackerData[NUM_OF_CAM]，其中NUM_OF_CAM为相机的个数，这意味这每一个相机都有一个FeatureTracker的实例，每个相机的FeatureTracker实例通过调用成员函数FeatureTracker::readIntrinsicParameter()，来读取每个相机各自对应的内参。&lt;/p&gt;

&lt;p&gt;特别的，如果相机是鱼眼相机，需要读取FISHEYE_MASK，存到相机FeatureTracker的实例的成员变量fisheye_mask中，它会在后续操作中被用来去除边缘噪点。&lt;/p&gt;

&lt;p&gt;接着定义一个订阅器和两个发布器。订阅器sub_img从话题IMAGE_TOPIC中订阅相机图像数据，回调函数为img_callback()。发布器pub_img在名为feature的话题下发布一条类型为sensor_msgs::PointCloud的消息，该话题消息为从相机图像中跟踪的特征点。发布器pub_match在名为feature_img的话题下发布一条类型为sensor_msgs::Image的消息，该话题消息为标记出了特征点的图像。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;name&lt;/th&gt;
&lt;th&gt;topic&lt;/th&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;th&gt;消息内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;subscriber&lt;/td&gt;
&lt;td&gt;sub_img&lt;/td&gt;
&lt;td&gt;IMAGE_TOPIC&lt;/td&gt;
&lt;td&gt;sensor_msgs::Image&lt;/td&gt;
&lt;td&gt;相机图像数据&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;publisher&lt;/td&gt;
&lt;td&gt;pub_img&lt;/td&gt;
&lt;td&gt;feature&lt;/td&gt;
&lt;td&gt;sensor_msgs::PointCloud&lt;/td&gt;
&lt;td&gt;跟踪的特征点&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;publisher&lt;/td&gt;
&lt;td&gt;pub_match&lt;/td&gt;
&lt;td&gt;feature_img&lt;/td&gt;
&lt;td&gt;sensor_msgs::Image&lt;/td&gt;
&lt;td&gt;标记出了特征点的图像&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;接着便循环等待回调函数，直至程序退出。&lt;/p&gt;

&lt;h1 id=&#34;回调函数img-callback&#34;&gt;回调函数img_callback()&lt;/h1&gt;

&lt;p&gt;前端的功能主要就在img_callback()，每当接收到从IMAGE_TOPIC话题订阅的数据，就会进入回调函数img_callback()进行处理。&lt;/p&gt;

&lt;h2 id=&#34;发布频率控制&#34;&gt;发布频率控制&lt;/h2&gt;

&lt;p&gt;并不是每处理一帧图像，都将特征点检测跟踪结果发布出去。数据发布频率由配置参数FREQ给定，通过PUB_THIS_FRAME控制是否发布当前帧的检测跟踪数据，将数据平均发布频率稳定在FREQ：如果当前统计时间内的平均数据发布频率快于FREQ，则将PUB_THIS_FRAME置为false，只进行特征点的跟踪，但不发布当前帧的数据；否则，将PUB_THIS_FRAME置为true，进行特征点的跟踪且发布当前帧的数据。&lt;/p&gt;

&lt;h2 id=&#34;特征点提取与光流跟踪&#34;&gt;特征点提取与光流跟踪&lt;/h2&gt;

&lt;p&gt;这一部分代码可处理单目相机和双目相机两种情况。&lt;/p&gt;

&lt;h3 id=&#34;单目处理逻辑&#34;&gt;单目处理逻辑&lt;/h3&gt;

&lt;p&gt;如果是单目相机（双目开关STEREO_TRACK为0），则只有一个相机：相机0。调用FeatureTracker::readImage()函数，读取单目图像数据，然后在readImage()函数中，对前一帧图像中的特征点进行金字塔光流跟踪，必要时检测新的特征点对特征点数量进行补充。&lt;/p&gt;

&lt;h3 id=&#34;双目处理逻辑&#34;&gt;双目处理逻辑&lt;/h3&gt;

&lt;p&gt;如果是双目相机（双目开关STEREO_TRACK为1），则有两个相机：相机0和相机1。对于相机0：在readImage()函数中，前后两帧图像之间进行金字塔光流跟踪，必要时在当前帧中检测新特征点以补充特征点数量。对于相机1：如果需要发布当前帧的数据（PUB_THIS_FRAME为true），且相机0的前一帧图像中特征点数量不为空，则直接在回调函数img_callback()中，相机1的当前帧图像对相机0的前一帧图像进行金字塔光流跟踪，这里光流跟踪的处理过程与单目模式下的类似，只是不会补充新的特征点；否则不需要进一步处理。&lt;/p&gt;

&lt;h2 id=&#34;featuretracker-readimage-函数&#34;&gt;FeatureTracker::readImage()函数&lt;/h2&gt;

&lt;p&gt;FeatureTracker类中的主要处理函数就是readImage()，在这个函数中涉及到几个变量名，需要对它们的含义进行特别说明（以下说明针对单目模式，其含义并不适用于双目模式），否则根据变量名称去揣测其含义会出错。&lt;/p&gt;

&lt;p&gt;图像数据变量：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;prev_img：  上一次发布数据时对应的图像帧&lt;/li&gt;
&lt;li&gt;cur_img：   光流跟踪的前一帧图像，而不是“当前帧”&lt;/li&gt;
&lt;li&gt;forw_img：  光流跟踪的后一帧图像，真正意义上的“当前帧”&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;特征点数据变量：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;prev_pts：  上一次发布的，且能够被当前帧（forw）跟踪到的特征点&lt;/li&gt;
&lt;li&gt;cur_pts：   在光流跟踪的前一帧图像中，能够被当前帧（forw）跟踪到的特征点&lt;/li&gt;
&lt;li&gt;forw_pts：  光流跟踪的后一帧图像，即当前帧中的特征点（除了跟踪到的特征点，可能还包含新检测的特征点）&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;FeatureTracker::readImage()函数的处理流程为：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;如果控制参数EQUALIZE为true，调用cv::createCLAHE对图像进行自适应直方图均衡处理；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;调用cv::calcOpticalFlowPyrLK()对前一帧的特征点cur_pts进行金字塔光流跟踪，得到forw_pts。status标记了cur_pts中各个特征点的跟踪状态，根据status将跟踪失败的特征点从prev_pts、cur_pts和forw_pts中剔除，而且在记录特征点id的ids，和记录特征点被跟踪次数的track_cnt中，也要把这些跟踪失败的特征点对应位置的记录删除。被status标记为跟踪正常的特征点，在当前帧图像中的位置可能已经处于图像边界外了，这些特征点也应该被删除，删除操作同上。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果不需要发布当前帧的数据，则直接将当前帧forw的相关数据赋给上一帧cur，然后在这一步整个readImage的流程就结束了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果需要发布当前帧的数据，先调用FeatureTracker::rejectWithF()函数，剔除outliers。具体方法为：调用cv::findFundamentalMat()对prev_pts和forw_pts计算F矩阵，通过F矩阵去除outliers。剩下的特征点track_cnt都加1。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;调用FeatureTracker::setMask()，通过设置一个mask，使跟踪的特征点在整幅图像中能够均匀分布，防止特征点扎堆。FeatureTracker::setMask()的具体操作为：对光流跟踪到的特征点forw_pts，按照被跟踪到的次数降序排列，然后按照降序遍历这些特征点。每选中一个特征点，在mask中将该点周围半径为MIN_DIST的区域设置为0，后面不再选取该区域内的特征点。这样会删去一些特征点，使得特征点分布得更加均匀，同时尽可能地保留被跟踪次数更多的特征点。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;由于光流跟踪到的特征点会减少，而且setMask()的处理过程中也会删除一些特征点，所以需要新检测一些特征点（只有需要发布数据时，才会检测新的特征点，否则只跟踪，不检测新的特征点）。具体操作为：调用cv::goodFeaturesToTrack()在mask中不为0的区域检测新的特征点，将特征点数量补充至指定数量。然后调用FeatureTracker::addPoints()，将新检测到的特征点到forw_pts中去，id初始化为-1，track_cnt初始化为1。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;更新特征点id&#34;&gt;更新特征点id&lt;/h2&gt;

&lt;p&gt;特征点id相当于特征点的身份证号，对数据关联（data association）至关重要。需要注意的是，更新特征点id的步骤被特意放到了回调函数img_callback()中，而不是FeatureTracker::readImage()函数内部。有一种说法是，n_id是FeatureTracker类的静态变量：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;k&#34;&gt;static&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n_id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;FeatureTracker类的多个实例对象会共享一个n_id，在readImage()函数内部更新特征点id的话，如果多个相机并行调用readImage()，它们都要去访问n_id并改变它的值，可能会产生问题。我有一个疑问：为什么会出现多个相机并行调用readImage()的情况，因为从源代码来说，可以保证多个相机的调用存在时序上的先后关系。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>slam相关资源汇总</title>
      <link>https://qingsimon.github.io/post/2018-09-26-slam%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/</link>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qingsimon.github.io/post/2018-09-26-slam%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/</guid>
      
        <description>&lt;p&gt;&lt;a href=&#34;https://www.cnblogs.com/shang-slam/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;徐尚-博客园&lt;/strong&gt;&lt;/a&gt;
各种开源vSLAM开源代码解读以及一些相关方面的笔记，其中关于VINS-Mono的一系列解读挺不错。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/q597967420/article/details/76099409&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;VINS-Mono源码解析&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>自动驾驶相关资源汇总</title>
      <link>https://qingsimon.github.io/post/2018-09-26-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/</link>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qingsimon.github.io/post/2018-09-26-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/</guid>
      
        <description>&lt;p&gt;&lt;a href=&#34;http://apollo.auto/devcenter/devcenter_cn.html
&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;优达学成与百度合作的自动驾驶入门课程，基于Apollo比较全面地介绍了自动驾驶&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.csdn.net/sinat_31135199/article/details/52926749&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;无人车、自动驾驶相关文章整理&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/c_147309339&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;无人驾驶干货铺&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.jianshu.com/p/12299e78e608&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;无人驾驶入门&amp;ndash;概述&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.jianshu.com/p/8121e5430ad9&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;无人驾驶入门&amp;ndash;高精度地图和定位&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.jianshu.com/p/260cf80025e5&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;无人驾驶入门&amp;ndash;感知&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.jianshu.com/p/d66f5a5e8bb6&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;无人驾驶入门&amp;ndash;预测&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.jianshu.com/p/07db1009a688&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;无人驾驶入门&amp;ndash;规划&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/27490744&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;自动驾驶技术综述论文简介&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;J. Janai, F. Güney, A. Behl and A. Geiger, “Computer vision for autonomous vehicles: problems, datasets and state-of-the-art”，arXiv:1704.05519v1, Apr. 2017.
这篇2017年4月18日发表于arXiv的论文来自KITTI团队。全文整整67页，不计参考文献与图片表格，正文文字部分也至少有50页。这篇论文更偏向于介绍各个方向的最新研究成果。他们还为这篇论文设计了一个神奇的网站&lt;a href=&#34;http://www.cvlibs.net/projects/autonomous_vision_survey/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;ISPRS 2017&lt;/strong&gt;&lt;/a&gt;。这个网页用放射结构将参考文献分门别类整理在一起，每一根射线都代表了一个研究方向。每一根射线都可以被点开，细分出该方向的重要参考文献。继续点击参考文献标题，会跳转至该文献的摘要并提供了pdf下载链接。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>高精度地图HD map</title>
      <link>https://qingsimon.github.io/post/2018-09-26-%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%9C%B0%E5%9B%BEhd-map/</link>
      <pubDate>Wed, 26 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qingsimon.github.io/post/2018-09-26-%E9%AB%98%E7%B2%BE%E5%BA%A6%E5%9C%B0%E5%9B%BEhd-map/</guid>
      
        <description>

&lt;h1 id=&#34;高精度地图概述&#34;&gt;高精度地图概述&lt;/h1&gt;

&lt;p&gt;高精度地图（HD map）是相对于常用的普通电子导航地图而言的，具有高精度、实时更新等特点。高精度地图不仅有高精度的坐标，同时还有准确的车道形状，并且每个车道的坡度、曲率、航向、高程、侧倾等数据也都含有。&lt;/p&gt;

&lt;h3 id=&#34;普通导航电子地图&#34;&gt;普通导航电子地图&lt;/h3&gt;

&lt;p&gt;当我们使用手机中的普通电子导航地图时，比如高德地图、谷歌地图、百度地图等时，手机通过GNSS定位的方式可以确定自己在地球上的大致位置，手机上的磁场感应模块可以判断出手机顶端的朝向（东南西北）。由于普通电子导航地图中存储着路标在地球上的位置，因此有了通过手机传感器获取的位置和方位信息，就能确定出手机在地图中的位置，这样我们就能通过手机导航软件去自己想去的地方。&lt;/p&gt;

&lt;p&gt;普通电子导航地图的存在一些不足：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;GNSS的定位误差较大,普通GNSS的定位误差通常在10m左右，所以有时候会感觉导航软件的定位结果挺蠢的，会将定位到自己附近的其它位置;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在建筑物密集的城市、隧道等场景下，GNSS信号有时可能会缺失，这会导致导航软件暂时无法完成定位工作；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;尽管普通电子地图中有很多道路、商铺的信息，这些信息还是相对简陋的。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;尽管普通电子地图有诸多不足，它还是基本上解决了人们在日常生活中的导航需求，这是因为人类能够从多种途径提取信息，并进行信息过滤，信息关联和推理联想，人类的信息综合处理能力弥补了普通电子地图的不足。比如对于定位不准的问题，导航软件将用户定位到了离用户10多米外的位置，人类用户可以在地图中查看软件定位结果周围一定范围内的地标，比如商铺、大楼等，并观察自己周围的真实环境，去寻找这些地标，人类用户能够通过这种综合处理能力修正定位结果。对于普通电子地图信息相对简陋的问题，比如地图中可能没有车道数量、车道线位置、车道的坡度等信息，但是人类用户的实时感知能力也可以弥补这些缺陷。简单来说，普通电子地图是供人类使用的地图，对精度没有苛刻的要求。&lt;/p&gt;

&lt;h3 id=&#34;高精度地图&#34;&gt;高精度地图&lt;/h3&gt;

&lt;p&gt;通常所说的高精度地图，是供车辆（计算机）使用的地图，而目前计算机的智能程度远远不及人类，计算机可以通过各种传感器获得大量的信息，但是它在信息过滤、信息关联、联想推理和泛化适应能力上，相比于人类还是十分幼稚的。因此，供计算机使用的地图要尽可能地给计算机提供非常直接的、高精度的信息。比如说十字路口的信息，对于人类来说，只需要告诉人类前方不远处有一个十字路口，人类就会提高警觉去观察周围环境，寻找十字路口，这种信息不需要很精确，因为即使告诉人类前方99.9m处有一个十字路口，这个比较精确的数字99.9对于人类的意义并不大，因为人类在处理过程中会把这个比较精确的信息转化为模糊信息“前方不远处”。很难想象，有哪个人开车的时候是在不断地计算自己与十字路口隔着多少米的距离。而对于车辆（计算机）来说，情况就不一样了，计算机恰恰需要精确的距离信息，而对于“前方不远处”这样的信息可能会无从下手。尽管现在的无人车辆通过多传感器融合和各种机器学习算法，对环境的实时感知能力有了很大的提升，但是其感知范围有一定限制，其感知能力也不能保证适应所有场景，在这种情况下，高精度地图可以为车辆提供高精度（分米级）的先验信息，这会大大缓解车辆实时感知的压力，降低其实时感知的难度。比如，有了高精度地图的辅助，车辆可以提前获取一公里以外的十字路口的位置和车道信息，尽管该路口并不在车辆的感知范围内。&lt;/p&gt;

&lt;h1 id=&#34;opendrive&#34;&gt;OpenDrive&lt;/h1&gt;

&lt;p&gt;Opendrive是一种开源的道路网逻辑描述的文件格式。最早用于驾驶模拟器中做路网文件的，与其相似的还有RoadXML。两种文件格式的说明及demo都可以从其官网下载。&lt;/p&gt;

&lt;h1 id=&#34;apollo高精度地图&#34;&gt;Apollo高精度地图&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://www.csdn.net/article/a/2018-04-12/15945514&#34; target=&#34;_blank&#34;&gt;Apollo高精地图解析&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/30728273&#34; target=&#34;_blank&#34;&gt;从百度Apollo代码谈高精度地图&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;momenta谈高精度地图&#34;&gt;Momenta谈高精度地图&lt;/h1&gt;

&lt;p&gt;以下内容均来自：&lt;a href=&#34;https://www.zhihu.com/question/268155924/answer/499947722&#34; target=&#34;_blank&#34;&gt;&lt;a href=&#34;https://www.zhihu.com/question/268155924/answer/499947722&#34;&gt;https://www.zhihu.com/question/268155924/answer/499947722&lt;/a&gt;&lt;/a&gt;&lt;br /&gt;
高精度地图是一个宽泛的概念,需要达到两方面的高精度，分别是地标位置的高精度和本车定位的高精度。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;地标位置的高精度。&lt;/strong&gt; 高精度地图由很多类地标构成，比如地面各种道路标线，地上各种交通标志等,地标的定义现在没有明确的标准,不同的厂商从自己的产品和技术需求出发,有不同 的定义方式。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;本车定位的高精度。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;视觉高精度地图的实现：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;视觉高精度地图的实现技术,不是SLAM，也不是SFM（我估计这里要表达的意思是：视觉高精度地图的实现技术，不是传统意义上的SLAM或者SFM）,这些方法都不适用。要建真正可用的视觉高精度地图，需要从第一原理出发,重新设计整个算法。我们构建高精度地图的第一原理是：多张图像存在视差,利用点的对应关系,可以从2D点恢复出3D点。从第一原理出发，建立高精度地图,需要创造性地解决三方面的问题:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;图像部分,检测识别语义点。&lt;/strong&gt; 传统SLAM或者SFM算法都基于SIFT，ORB等人工设计的特征点，这些特征点在光照、视角发生变化的情况下,无法进行准确的检测和匹配。换句话说，光照视角发生变化后，原来构建的地图就无法使用了。我们的方法是定义道路标线、标牌等地标上的点作为语义点，通过深度学习和数据驱动的监督训练得到模型，可以准确检测和识别语义点，解决了检测不到、匹配错误的问题。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;几何部分，通过众包间接实现海量摄像头测量效果。&lt;/strong&gt; 不同车辆，不同时间，经过同一路标，即使光照视角不同，我们也可以通过语义点模型把所有车辆拍摄到的同一语义点关联起来，这相当于间接实现了多摄像头测距的效果。我们知道，视觉测量中，摄像头越多，视差覆盖越全，测量精度就越高。我们实验验证，随着众包车次的增加，真实3D点位置估计的准确性有量级上的提升。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;GPS部分，给每个语义点精确的GPS坐标。&lt;/strong&gt; 我们有几方面的考虑：（1）让高精度地图通用。GPS坐标是地图的通用语言，给每个语义点赋予GPS坐标，便于他人使用；（2）消除累积误差。单纯使用几何方法构建局部地图，会有累积误差，结合GPS可以解决这个问题；（3）消除局部地图歧义性。当局部地图有重合或者语义点缺失的时候，确定局部地图坐标系很麻烦（暂时不太明白这是什么意思），但全局GPS坐标系没有这个问题。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;a-href-https-mp-weixin-qq-com-s-biz-mzi1odywotkzng-mid-2247488876-idx-1-sn-45f01456476ee71ea657d7b563935874-chksm-ea04d313dd735a05dbd4c8893287f3176edaefc15f44f35192128ef8ab1c04a84dc3e257e8ce-mpshare-1-scene-1-srcid-1013eytiyryrggkjjreizg7o-rd-target-blank-高精地图-无人驾驶领域的稀缺资源-a&#34;&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzI1ODYwOTkzNg==&amp;mid=2247488876&amp;idx=1&amp;sn=45f01456476ee71ea657d7b563935874&amp;chksm=ea04d313dd735a05dbd4c8893287f3176edaefc15f44f35192128ef8ab1c04a84dc3e257e8ce&amp;mpshare=1&amp;scene=1&amp;srcid=1013EytiYRyRggKjJREIzG7o#rd&#34; target=&#34;_blank&#34;&gt;高精地图:无人驾驶领域的稀缺资源&lt;/a&gt;&lt;/h2&gt;

&lt;h2 id=&#34;高精度地图制作资质&#34;&gt;高精度地图制作资质&lt;/h2&gt;

&lt;p&gt;高精度地图不是谁想做都能做的，首先得让&lt;strong&gt;国家测绘地理信息局&lt;/strong&gt;在你的屁股上盖一个小红戳。
&lt;a href=&#34;https://www.sohu.com/a/202385620_335896&#34; target=&#34;_blank&#34;&gt;第14家图商名落滴滴，甲级导航地图资质已放开？&lt;/a&gt;&lt;br /&gt;
和美国不同，我国有比较严格的地图测绘政策限制，目前拥有「导航电子地图资质单位名单」的企业有 14 家。分别是四维图新、高德、长地万方、凯立德、易图通、城际高科、国家基础地理信息中心、科菱航睿、光庭信息、浙江省第一测绘院、江苏省基础地理信息中心、灵图、立得空间信息、滴图科技。百度地图就是通过子公司长地万方开展导航电子地图测绘的。&lt;/p&gt;

&lt;h2 id=&#34;mobileye众包模式-vs-google集中模式&#34;&gt;Mobileye众包模式 VS Google集中模式&lt;/h2&gt;

&lt;p&gt;目前主流的高精度地图数据采集主要包括以Mobileye为代表的众包模式和以Google、高德为代表的集中制图模式。&lt;/p&gt;

&lt;h2 id=&#34;静态高精度地图-动态高精度地图&#34;&gt;静态高精度地图 + 动态高精度地图&lt;/h2&gt;
</description>
      
    </item>
    
    <item>
      <title>自动驾驶相关公司整理</title>
      <link>https://qingsimon.github.io/post/2018-09-25-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%9B%B8%E5%85%B3%E5%85%AC%E5%8F%B8%E6%95%B4%E7%90%86/</link>
      <pubDate>Tue, 25 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://qingsimon.github.io/post/2018-09-25-%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E7%9B%B8%E5%85%B3%E5%85%AC%E5%8F%B8%E6%95%B4%E7%90%86/</guid>
      
        <description>

&lt;p&gt;&lt;strong&gt;自动驾驶相关公司：&lt;/strong&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
    &lt;img src=&#34;https://qingsimon.github.io/img/自动驾驶相关公司.jpg&#34; alt=&#34;&#34; width=&#34;800&#34;/&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;图片来源&lt;a href=&#34;http://www.prweb.com/releases/vsi/segmentsautonomousvehicle/prweb13472308.htm&#34; target=&#34;_blank&#34;&gt;Segmenting the Autonomous Vehicle Value Chain: A Look at Who is in the “Driverless&amp;rdquo; Seat&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自动驾驶专利相关数量：&lt;/strong&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
    &lt;img src=&#34;https://qingsimon.github.io/img/autonomous_driving_patents.jpg&#34; alt=&#34;&#34; width=&#34;800&#34;/&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;图片来源&lt;a href=&#34;https://www.statista.com/chart/10879/autonomous-driving-patents/&#34; target=&#34;_blank&#34;&gt;&lt;a href=&#34;https://www.statista.com/chart/10879/autonomous-driving-patents/&#34;&gt;https://www.statista.com/chart/10879/autonomous-driving-patents/&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;从自动驾驶相关专利数量来看，传统车企是闷声发大财。这些传统车企资金雄厚，积淀了很多年的口碑，控制着供应和销售的渠道，如果自动驾驶真的在未来有所作为的话，它似乎也革不了这些传统车企的命，现在的自动驾驶初创公司也积极抱紧这些大腿，寻求与它们的合作。&lt;/p&gt;

&lt;p&gt;实际上现在的自动驾驶公司要么积极寻求与车企和供应商的合作，要么被它们收购。
合作：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://techcrunch.com/2017/05/16/bmw-intel-and-mobileye-bring-delphi-in-on-their-self-driving-platform/&#34; target=&#34;_blank&#34;&gt;BMW + Intel + Mobileye + Delphi&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://autoweek.com/article/autonomous-cars/volvo-continues-support-uber-after-its-self-driving-accident&#34; target=&#34;_blank&#34;&gt;Volvo + Uber&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;收购：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.autosphere.ca/collisionmanagement/collision-news/2017/02/14/ford-investing-argo-ai/&#34; target=&#34;_blank&#34;&gt;Ford + Argo&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.cheyun.com/content/9894&#34; target=&#34;_blank&#34;&gt;GM + Cruise&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;车厂和零部件供应商&#34;&gt;车厂和零部件供应商&lt;/h1&gt;

&lt;h2 id=&#34;国内公司&#34;&gt;国内公司&lt;/h2&gt;

&lt;h3 id=&#34;陕汽集团&#34;&gt;陕汽集团&lt;/h3&gt;

&lt;p&gt;与图森未来合作&lt;/p&gt;

&lt;h3 id=&#34;广汽集团&#34;&gt;广汽集团&lt;/h3&gt;

&lt;p&gt;与小马智行合作&lt;/p&gt;

&lt;h2 id=&#34;国外公司&#34;&gt;国外公司&lt;/h2&gt;

&lt;h3 id=&#34;博世-bosch&#34;&gt;博世（Bosch）&lt;/h3&gt;

&lt;h3 id=&#34;奥迪-audi&#34;&gt;奥迪（Audi）&lt;/h3&gt;

&lt;h3 id=&#34;continental&#34;&gt;Continental&lt;/h3&gt;

&lt;p&gt;德国大陆集团（Continental AG），创始于1871年，是世界先进的汽车配套产品供应商之一。&lt;/p&gt;

&lt;h3 id=&#34;福特-ford&#34;&gt;福特（Ford）&lt;/h3&gt;

&lt;p&gt;2017年2月，福特对Argo投资了10亿美元。&lt;/p&gt;

&lt;h3 id=&#34;通用汽车-gm&#34;&gt;通用汽车（GM）&lt;/h3&gt;

&lt;p&gt;通用在2016年1月投资5亿美元给Lyft（美国第二大移动出行公司），此后在2016年3月收购了无人驾驶初创企业Cruise，Cruise会作为通用的一个独立部门而存在。&lt;/p&gt;

&lt;h3 id=&#34;宝马-bmw&#34;&gt;宝马（BMW）&lt;/h3&gt;

&lt;h3 id=&#34;丰田-toyota&#34;&gt;丰田（Toyota）&lt;/h3&gt;

&lt;h3 id=&#34;大众-volkswagen&#34;&gt;大众（Volkswagen）&lt;/h3&gt;

&lt;h3 id=&#34;戴姆勒-daimler&#34;&gt;戴姆勒（Daimler）&lt;/h3&gt;

&lt;h3 id=&#34;沃尔沃-volvo&#34;&gt;沃尔沃（Volvo）&lt;/h3&gt;

&lt;h3 id=&#34;delphi&#34;&gt;Delphi&lt;/h3&gt;

&lt;p&gt;可能很多人都不知道Delphi是一家什么公司，反正我一开始是没听说过的。Delphi是一家历史久远的汽车零部件供应商&lt;/p&gt;

&lt;h3 id=&#34;tesla&#34;&gt;Tesla&lt;/h3&gt;

&lt;p&gt;Tesla算是新兴的整车厂。&lt;/p&gt;

&lt;h1 id=&#34;自动驾驶解决方案与服务提供商&#34;&gt;自动驾驶解决方案与服务提供商&lt;/h1&gt;

&lt;h2 id=&#34;国内公司-1&#34;&gt;国内公司&lt;/h2&gt;

&lt;h3 id=&#34;地平线&#34;&gt;地平线&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.horizon.ai/&#34; target=&#34;_blank&#34;&gt;地平线&lt;/a&gt;总给我一种Mobileye的感觉，他们在搞芯片，我觉的似乎是对标Mobileye的EyeQ系列芯片。&lt;/p&gt;

&lt;h3 id=&#34;图森未来&#34;&gt;图森未来&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.tusimple.com/index.html&#34; target=&#34;_blank&#34;&gt;图森未来&lt;/a&gt;成立于2015年，2015年9月完成A轮5000万人民币融资，2017年3月与陕汽集团达成战略合作伙伴关系。2017年8月完成来自新浪、英伟达和治平资本的共计2300万美元的B轮融资。&lt;/p&gt;

&lt;p&gt;2018年9月，我听了一个图森未来的知乎LIVE，似乎图森未来自己在做摄像头，使得无人货车夜间也能在港口工作。图森未来以商用卡车作为切入点，在封闭环境和限定场景下，有望率先将自动驾驶应用落地，按照他们的说法，他们已经有收入了。&lt;/p&gt;

&lt;h3 id=&#34;momenta&#34;&gt;Momenta&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://www.momenta.cn/&#34; target=&#34;_blank&#34;&gt;Momenta&lt;/a&gt;成立于2016年&lt;/p&gt;

&lt;h3 id=&#34;小马智行-pony-ai&#34;&gt;小马智行（pony.ai）&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.pony.ai/zh/&#34; target=&#34;_blank&#34;&gt;小马智行（pony.ai）&lt;/a&gt;2016年12月成立，2018年1月宣布完成1.12亿美元A轮融资，2018年2月与广汽集团签订战略合作协议。&lt;/p&gt;

&lt;h3 id=&#34;roadstar-ai&#34;&gt;Roadstar.ai&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://roadstar.ai/&#34; target=&#34;_blank&#34;&gt;Roadstar.ai&lt;/a&gt;的2018年的招聘宣讲会我听过一次，印象比较深的就是他们很强调传感器在时间和空间上的高精度同步，他们还展示了传感器前融合的技术DeepFusion，即在原始数据层面的融合（其实我是懵b的）。
&lt;a href=&#34;https://zhuanlan.zhihu.com/p/37866364&#34; target=&#34;_blank&#34;&gt;多传感器前融合&lt;/a&gt;对Roadstar的传感器前融合技术DeepFusion有较为详细的介绍。
另外，RoadStar在宣讲中还提到了一点，他们正在自己做激光雷达。&lt;/p&gt;

&lt;h3 id=&#34;deepmotion&#34;&gt;DeepMotion&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.deepmotion.ai/&#34; target=&#34;_blank&#34;&gt;DeepMotion&lt;/a&gt;是为自动驾驶提供众包高精度地图解决方案的公司，提供10厘米级别的三位语义地图及定位。&lt;/p&gt;

&lt;h3 id=&#34;景驰科技&#34;&gt;景驰科技&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://jingchi.ai/&#34; target=&#34;_blank&#34;&gt;景驰科技&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;智加&#34;&gt;智加&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://plus.ai/cn/&#34; target=&#34;_blank&#34;&gt;智加&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;cidi&#34;&gt;CiDi&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://biz.jrj.com.cn/2018/04/19164324417362.shtml&#34; target=&#34;_blank&#34;&gt;物流自动驾驶汽车CIDI上路&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;西井科技&#34;&gt;西井科技&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.westwell-lab.com/&#34; target=&#34;_blank&#34;&gt;西井科技&lt;/a&gt;&lt;/p&gt;

&lt;!-- 从新窗口中打开链接 --&gt;

&lt;!-- ## &lt;a href=&#34;网址&#34; target=&#34;_blank&#34;&gt;显示字符&lt;/a&gt; --&gt;

&lt;h2 id=&#34;国外公司-1&#34;&gt;国外公司&lt;/h2&gt;

&lt;h3 id=&#34;mobileye&#34;&gt;mobileye&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.mobileye.com/&#34; target=&#34;_blank&#34;&gt;mobileye&lt;/a&gt;可谓是ADAS界的祖师爷了。&lt;/p&gt;

&lt;p&gt;Mobileye由以色列希伯来大学的Amnon Shashua教授和Ziv Aviram于1999年创立。自创立之日起，Mobileye就将公司定位为开发和推广一个视觉系统，以协助驾驶员在驾驶过程中保障乘客安全和减少交通事故。由于起步早，经过多年积累，Mobileye在单目高级驾驶辅助系统（ADAS）的开发方面走在世界前列。公司提供芯片（EyeQ芯片）搭载系统和计算机视觉算法运行DAS客户端功能，如车道偏离警告（LDA），基于雷达视觉融合的车辆探测、前部碰撞警告（FCW），车距检测（HMW）、行人探测、智能前灯控制（IHC），交通标志识别（TSR），仅视觉自适应巡航控制（ACC）等。2017年3月3日，英特尔正式宣布，以每股63.54美元现金收购Mobileye，股权价值约为153亿美金。Mobile作为ADAS界的大佬，占据了汽车安全驾驶系统全球90%以上的市场份额。&lt;/p&gt;

&lt;p&gt;Mobileye的CEO Shashua在CVPR2016上介绍了REM（Road Experience Management，道路管理系统），目前仍然是视觉高精度地图和定位（几乎）唯一的解决方案。REM可以实时更新高精地图信息，REM每公里产生的数据量为10kB，极大地降低了云端高精地图的更新成本。&lt;/p&gt;

&lt;p&gt;Mobileye开发的EyeQ系列车载芯片，由意法半导体生产供应，EyeQ3就是第一代特斯拉Autopilot背后的大脑。&lt;/p&gt;

&lt;p&gt;这两年间，Mobileye拉拢若干主机厂加入REM的联盟，将于今年推出EyeQ4，真正开始REM的量产之路。国内也不乏REM的追随者，如Momenta、DeepMotion等。Mobileye在2016年和2017年申请了一批sparse map、crowd sourcing、navigation相关的专利，内容大同小异。这些专利意在保护整个REM产品，覆盖全面，但技术细节很少提及。&lt;/p&gt;

&lt;p&gt;2017年3月，英特尔用约153亿美元收购了Mobileye，要和英伟达刚正面。&lt;/p&gt;

&lt;h3 id=&#34;waymo&#34;&gt;waymo&lt;/h3&gt;

&lt;p&gt;说到&lt;a href=&#34;https://waymo.com/&#34; target=&#34;_blank&#34;&gt;waymo&lt;/a&gt;不得不提起DARPA Grand Challenge 2005，该赛事由美国国防部（United States Department of Defense）举办，受DARPA（Defense Advanced Research Projects Agency）资助，在2005年举办第二届。斯坦福人工智能实验室（the Stanford Artificial Intelligence Laboratory）的director Sebastian Thrun（他也是Google Street View的联合创始人）带领stanford的团队研发的自动驾驶汽车Stanley，赢得了DARPA Grand Challenge 2005，并获得了200万美元的奖金，后来Sebastian Thrun的团队被google收编。2009年，google开启了一项自动驾驶汽车计划，2016年，自动驾驶部门从google独立出来，成立了现在的waymo。&lt;/p&gt;

&lt;h3 id=&#34;tesla-1&#34;&gt;tesla&lt;/h3&gt;

&lt;h3 id=&#34;nutonomy&#34;&gt;NuTonomy&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.nutonomy.com/&#34; target=&#34;_blank&#34;&gt;Nutonomy&lt;/a&gt;成立于2013年，2016年8月，Nutonomy在新加坡开始提供robo-taxi服务。公司CTO Emilio Frazzoli在MIT的自动驾驶讲座&lt;a href=&#34;https://www.bilibili.com/video/av23594594/?p=7&#34; target=&#34;_blank&#34;&gt;中文字幕版视频&lt;/a&gt;和&lt;a href=&#34;https://www.bilibili.com/video/av18945043/?p=7&#34; target=&#34;_blank&#34;&gt;英文字幕版视频&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;aurora&#34;&gt;Aurora&lt;/h3&gt;

&lt;p&gt;Sterling Anderson是Aurora的联合创始人，Anderson之前是Tesla自动驾驶团队的leader，带领团队成功开发和运作了Tesla的第一代、第二代自动驾驶汽车。Aurora成立于2016年12月，目前已经与大众和现代建立了商业合作。&lt;/p&gt;

&lt;h3 id=&#34;uber-otto&#34;&gt;uber + otto&lt;/h3&gt;

&lt;p&gt;2016年8月，uber用6.8亿美元收购otto，otto是一家做自动驾驶卡车的公司。&lt;/p&gt;

&lt;h3 id=&#34;cruise&#34;&gt;Cruise&lt;/h3&gt;

&lt;h3 id=&#34;autox&#34;&gt;AutoX&lt;/h3&gt;

&lt;h1 id=&#34;图商&#34;&gt;图商&lt;/h1&gt;

&lt;h2 id=&#34;国内公司-2&#34;&gt;国内公司&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;国内图商：&lt;/strong&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
    &lt;img src=&#34;https://qingsimon.github.io/img/国内图商.png&#34; alt=&#34;&#34; width=&#34;800&#34;/&gt;
&lt;/div&gt;&lt;/p&gt;

&lt;h3 id=&#34;百度-长地万方&#34;&gt;百度 + 长地万方&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.csdn.net/article/a/2018-04-12/15945514&#34; target=&#34;_blank&#34;&gt;Apollo高精地图解析&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/30728273&#34; target=&#34;_blank&#34;&gt;从百度Apollo代码谈高精度地图&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;腾讯-科菱航睿&#34;&gt;腾讯 + 科菱航睿&lt;/h3&gt;

&lt;h3 id=&#34;腾讯-四维图新&#34;&gt;腾讯 + 四维图新&lt;/h3&gt;

&lt;h3 id=&#34;阿里-高德地图&#34;&gt;阿里 + 高德地图&lt;/h3&gt;

&lt;h3 id=&#34;阿里-易图通&#34;&gt;阿里 + 易图通&lt;/h3&gt;

&lt;h3 id=&#34;小米-凯立德&#34;&gt;小米 + 凯立德&lt;/h3&gt;

&lt;h3 id=&#34;滴滴&#34;&gt;滴滴&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.sohu.com/a/202385620_335896&#34; target=&#34;_blank&#34;&gt;第14家图商名落滴滴，甲级导航地图资质已放开？&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;国外公司-2&#34;&gt;国外公司&lt;/h2&gt;

&lt;h3 id=&#34;here&#34;&gt;Here&lt;/h3&gt;

&lt;p&gt;Here前身为美国地图公司NAVTEQ，2008年诺基亚以81亿美元的价格收购NAVTEQ，并入自己的Here地图部门。2015年4月，诺基亚宣布剥离Here部门，吸引了宝马、奔驰、奥迪、Google、Uber、Facebook、百度、腾讯（联合四维图新）来竞标，最终宝马、奔驰、奥迪临时组成的联合体以28亿欧元全资收购Here，三家各占1/3的股权。Here地图数据覆盖约200个国家，超过4600万公里（这是个什么概念呢？2017年末，中国全国公路总里程477.35万公里，高速公路13.65万公里。其实我比较好奇，4600万公里，全世界有这么多公路吗？）。&lt;/p&gt;

&lt;h3 id=&#34;tomtom&#34;&gt;TomTom&lt;/h3&gt;

&lt;h1 id=&#34;车载芯片&#34;&gt;车载芯片&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;AI芯片相关公司：&lt;/strong&gt;
&lt;div style=&#34;text-align:center&#34;&gt;
    &lt;img src=&#34;https://qingsimon.github.io/img/AI芯片相关公司.jpg&#34; alt=&#34;&#34; width=&#34;800&#34;/&gt;
&lt;/div&gt;
根据Compass Intelligence对全球100多家AI芯片企业进行的排名显示，华为海思（HiSilicon）位列榜单第12名，联发科（Media Tek）位列第14名，Imagination位列第15名，瑞芯微（Rockchip）位列第20名，芯原（Verisilcon）位列第21名，寒武纪（combricon）位列第23名，地平线机器人（Horizon Robotics）位列第24名。&lt;/p&gt;
</description>
      
    </item>
    
  </channel>
</rss>